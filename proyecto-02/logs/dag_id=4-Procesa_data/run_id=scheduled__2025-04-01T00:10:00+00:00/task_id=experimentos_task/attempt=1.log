[2025-04-02T02:35:02.807+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-02T02:35:02.846+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [queued]>
[2025-04-02T02:35:02.860+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [queued]>
[2025-04-02T02:35:02.860+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-04-02T02:35:02.890+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): experimentos_task> on 2025-04-01 00:10:00+00:00
[2025-04-02T02:35:03.027+0000] {standard_task_runner.py:72} INFO - Started process 145 to run task
[2025-04-02T02:35:03.079+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', '4-Procesa_data', 'experimentos_task', 'scheduled__2025-04-01T00:10:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/Procesa_data.py', '--cfg-path', '/tmp/tmpiun3z82k']
[2025-04-02T02:35:03.089+0000] {standard_task_runner.py:105} INFO - Job 5: Subtask experimentos_task
[2025-04-02T02:35:03.485+0000] {task_command.py:467} INFO - Running <TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [running]> on host 66352f2efff5
[2025-04-02T02:35:03.946+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='4-Procesa_data' AIRFLOW_CTX_TASK_ID='experimentos_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-01T00:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-01T00:10:00+00:00'
[2025-04-02T02:35:03.953+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-02T02:35:04.091+0000] {warnings.py:109} WARNING - /opt/***/dags/Procesa_data.py:67: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(query, conn)

[2025-04-02T02:35:04.175+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/Procesa_data.py", line 83, in experimentar
    X = df_covertype.drop(columns=['Cover_Type'])  # Features
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/frame.py", line 5258, in drop
    return super().drop(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 4549, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 4591, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 6699, in drop
    raise KeyError(f"{list(labels[mask])} not found in axis")
KeyError: "['Cover_Type'] not found in axis"
[2025-04-02T02:35:04.249+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=4-Procesa_data, task_id=experimentos_task, run_id=scheduled__2025-04-01T00:10:00+00:00, execution_date=20250401T001000, start_date=20250402T023502, end_date=20250402T023504
[2025-04-02T02:35:04.391+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-02T02:35:04.394+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 5 for task experimentos_task ("['Cover_Type'] not found in axis"; 145)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/Procesa_data.py", line 83, in experimentar
    X = df_covertype.drop(columns=['Cover_Type'])  # Features
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/frame.py", line 5258, in drop
    return super().drop(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 4549, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 4591, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 6699, in drop
    raise KeyError(f"{list(labels[mask])} not found in axis")
KeyError: "['Cover_Type'] not found in axis"
[2025-04-02T02:35:04.485+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-02T02:35:04.549+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-02T02:35:04.553+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-02T18:03:15.497+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-02T18:03:15.534+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [queued]>
[2025-04-02T18:03:15.550+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [queued]>
[2025-04-02T18:03:15.551+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-04-02T18:03:15.572+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): experimentos_task> on 2025-04-01 00:10:00+00:00
[2025-04-02T18:03:15.604+0000] {standard_task_runner.py:72} INFO - Started process 320 to run task
[2025-04-02T18:03:15.614+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', '4-Procesa_data', 'experimentos_task', 'scheduled__2025-04-01T00:10:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/Procesa_data.py', '--cfg-path', '/tmp/tmpl8kuyphx']
[2025-04-02T18:03:15.621+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask experimentos_task
[2025-04-02T18:03:15.742+0000] {task_command.py:467} INFO - Running <TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [running]> on host 0f069c6cac7c
[2025-04-02T18:03:15.898+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='4-Procesa_data' AIRFLOW_CTX_TASK_ID='experimentos_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-01T00:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-01T00:10:00+00:00'
[2025-04-02T18:03:15.903+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-02T18:03:15.988+0000] {warnings.py:109} WARNING - /opt/***/dags/Procesa_data.py:67: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(query, conn)

[2025-04-02T18:03:19.550+0000] {logging_mixin.py:190} WARNING - 2025/04/02 18:03:19 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.
The git executable must be specified in one of the following ways:
    - be included in your $PATH
    - be set via $GIT_PYTHON_GIT_EXECUTABLE
    - explicitly set via git.refresh(<full-path-to-git-executable>)

All git commands will error until this is rectified.

This initial message can be silenced or aggravated in the future by setting the
$GIT_PYTHON_REFRESH environment variable. Use one of the following values:
    - quiet|q|silence|s|silent|none|n|0: for no message or exception
    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)
    - error|e|exception|raise|r|2: for a raised exception

Example:
    export GIT_PYTHON_REFRESH=quiet
[2025-04-02T18:03:19.657+0000] {logging_mixin.py:190} WARNING - 2025/04/02 18:03:19 WARNING mlflow.utils: Truncated the value of the key `estimator`. Truncated value: `Pipeline(steps=[('column_trans',
                 ColumnTransformer(remainder='passthrough',
                                   transformers=[('onehotencoder',
                                                  OneHotEncoder(handle_unknown='ignore'),
                                                  ['Wilderness_Area',
                                                   'Soil_Type'])])),
                ('scaler', StandardScaler(with_mean=False)),
                ('RandomForestClassifier', Rand...`
[2025-04-02T18:03:19.678+0000] {logging_and_warnings.py:72} WARNING - /home/***/.local/lib/python3.8/site-packages/joblib/parallel.py:1359: UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1
  n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,

[2025-04-02T18:03:48.759+0000] {logging_and_warnings.py:72} WARNING - /home/***/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

[2025-04-02T18:03:51.798+0000] {credentials.py:1147} INFO - Found credentials in environment variables.
[2025-04-02T18:03:53.484+0000] {logging_mixin.py:190} WARNING - 2025/04/02 18:03:53 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: "/home/***/.local/lib/python3.8/site-packages/mlflow/models/signature.py:137: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details."
[2025-04-02T18:04:04.577+0000] {logging_mixin.py:190} WARNING - Registered model 'modelo1' already exists. Creating a new version of this model...
[2025-04-02T18:04:04.694+0000] {logging_mixin.py:190} WARNING - 2025/04/02 18:04:04 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: modelo1, version 3
[2025-04-02T18:04:04.700+0000] {logging_mixin.py:190} WARNING - Created version '3' of model 'modelo1'.
[2025-04-02T18:04:13.188+0000] {logging_mixin.py:190} WARNING - 2025/04/02 18:04:13 INFO mlflow.sklearn.utils: Logging the 5 best runs, 3 runs will be omitted.
[2025-04-02T18:04:14.351+0000] {logging_mixin.py:190} INFO - Experimento registrado correctamente.
[2025-04-02T18:04:14.375+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-04-02T18:04:14.399+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-02T18:04:14.400+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=4-Procesa_data, task_id=experimentos_task, run_id=scheduled__2025-04-01T00:10:00+00:00, execution_date=20250401T001000, start_date=20250402T180315, end_date=20250402T180414
[2025-04-02T18:04:14.620+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-02T18:04:14.654+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-02T18:04:14.655+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-02T22:13:05.054+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-02T22:13:05.084+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [queued]>
[2025-04-02T22:13:05.094+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [queued]>
[2025-04-02T22:13:05.095+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-04-02T22:13:05.113+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): experimentos_task> on 2025-04-01 00:10:00+00:00
[2025-04-02T22:13:05.137+0000] {standard_task_runner.py:72} INFO - Started process 541 to run task
[2025-04-02T22:13:05.149+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', '4-Procesa_data', 'experimentos_task', 'scheduled__2025-04-01T00:10:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/Procesa_data.py', '--cfg-path', '/tmp/tmpv2auqsev']
[2025-04-02T22:13:05.157+0000] {standard_task_runner.py:105} INFO - Job 20: Subtask experimentos_task
[2025-04-02T22:13:05.268+0000] {task_command.py:467} INFO - Running <TaskInstance: 4-Procesa_data.experimentos_task scheduled__2025-04-01T00:10:00+00:00 [running]> on host 007bc2195d27
[2025-04-02T22:13:05.414+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='4-Procesa_data' AIRFLOW_CTX_TASK_ID='experimentos_task' AIRFLOW_CTX_EXECUTION_DATE='2025-04-01T00:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-01T00:10:00+00:00'
[2025-04-02T22:13:05.416+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-02T22:13:05.479+0000] {warnings.py:109} WARNING - /opt/***/dags/Procesa_data.py:67: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(query, conn)

[2025-04-02T22:13:05.548+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/Procesa_data.py", line 87, in experimentar
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  File "/home/airflow/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 2420, in train_test_split
    n_train, n_test = _validate_shuffle_split(
  File "/home/airflow/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 2098, in _validate_shuffle_split
    raise ValueError(
ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.
[2025-04-02T22:13:05.579+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=4-Procesa_data, task_id=experimentos_task, run_id=scheduled__2025-04-01T00:10:00+00:00, execution_date=20250401T001000, start_date=20250402T221305, end_date=20250402T221305
[2025-04-02T22:13:05.637+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-02T22:13:05.638+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 20 for task experimentos_task (With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.; 541)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 3006, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 274, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 3161, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 3185, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 768, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 734, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 424, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/dags/Procesa_data.py", line 87, in experimentar
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  File "/home/airflow/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 2420, in train_test_split
    n_train, n_test = _validate_shuffle_split(
  File "/home/airflow/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 2098, in _validate_shuffle_split
    raise ValueError(
ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.
[2025-04-02T22:13:05.681+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-02T22:13:05.721+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-02T22:13:05.724+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
