# Poryecto-01: Entorno de Desarrollo para Machine Learning  

Este proyecto tiene como objetivo construir un ambiente de desarrollo para machine learning, que permita la ingesta, validaciÃ³n y transformaciÃ³n de datos. Para esto se creo un ambiente de desarrollo de jupyterlab en un contenedor docker basado en la imagen python:3.9-slim-bookworm, este ambiente se despliega en el escritorio de la mÃ¡quina virtual **10.43.101.206**.

## ğŸ“ Estructura del Repositorio

```plaintext
proyecto-01/
â”œâ”€â”€ train/                      # ğŸ“ Carpeta principal del entrenamiento
â”‚   â”œâ”€â”€ Dockerfile              # ğŸ³ ConfiguraciÃ³n del contenedor de entrenamiento
â”‚   â”œâ”€â”€ __pycache__/            # ğŸ—„ï¸ CachÃ© de archivos compilados
â”‚   â”œâ”€â”€ data/                   # ğŸ“‚ Conjunto de datos
â”‚   â”‚   â”œâ”€â”€ covertype/          
â”‚   â”‚   â”‚   â”œâ”€â”€ covertype_train.csv   # ğŸ“„ Datos de entrenamiento
â”‚   â”‚   â”‚   â”œâ”€â”€ serving.csv           # ğŸ“„ Datos de prueba
â”‚   â”‚   â”œâ”€â”€ data_new/         
â”‚   â”‚   â”‚   â”œâ”€â”€ cover_new.csv         # ğŸ“„ Datos nuevos
â”‚   â”‚   â”œâ”€â”€ data_service/     
â”‚   â”‚   â”‚   â”œâ”€â”€ service_data.csv      # ğŸ“„ Datos del servicio
â”‚   â”œâ”€â”€ metadata/               # ğŸ›ï¸ Almacenamiento de metadatos
â”‚   â”‚   â”œâ”€â”€ metadata.sqlite
â”‚   â”‚   â”œâ”€â”€ metadata.db
â”‚   â”œâ”€â”€ pipeline_outputs/        # ğŸ”„ Salida del pipeline de datos
â”‚   â”‚   â”œâ”€â”€ CsvExampleGen/
â”‚   â”‚   â”œâ”€â”€ _wheels/
â”‚   â”œâ”€â”€ pipeline_root/           # ğŸ—ï¸ RaÃ­z del pipeline
â”‚   â”œâ”€â”€ schema/                  # ğŸ“‘ Esquemas del dataset
â”‚   â”‚   â”œâ”€â”€ schema.pb
â”‚   â”‚   â”œâ”€â”€ schema.pbtxt
â”‚   â”‚   â”œâ”€â”€ schema_with_envs.pbtxt
â”‚   â”œâ”€â”€ data_preparation.py       # ğŸ”¢ PreparaciÃ³n de datos
â”‚   â”œâ”€â”€ model_creation.py         # ğŸ” CreaciÃ³n del modelo
â”‚   â”œâ”€â”€ preprocessing.py          # âš™ï¸ Preprocesamiento de datos
â”‚   â”œâ”€â”€ proyecto-01.ipynb         # ğŸ“Š Notebook con el desarrollo del proyecto
â”‚   â”œâ”€â”€ requirements.txt          # ğŸ“¦ Dependencias del proyecto
â”‚
â”œâ”€â”€ docker-compose.yml            # ğŸ³ ConfiguraciÃ³n de Docker
â”œâ”€â”€ jupyterlab.service            # ğŸ”§ ConfiguraciÃ³n del servicio JupyterLab
â”œâ”€â”€ jupyterlab.sh                 # ğŸ–¥ï¸ Script para iniciar JupyterLab
â”œâ”€â”€ readme.md                     # ğŸ“œ DocumentaciÃ³n del proyecto
```
## ğŸ”¹ CaracterÃ­sticas del Proyecto  

El entorno de desarrollo construido permitirÃ¡ ejecutar un flujo de datos con las siguientes etapas:  
- **SelecciÃ³n de caracterÃ­sticas**  
- **Ingesta del conjunto de datos**  
- **GeneraciÃ³n de estadÃ­sticas del dataset**  
- **CreaciÃ³n de un esquema basado en el conocimiento del dominio**  
- **DefiniciÃ³n y creaciÃ³n de entornos de esquema**  
- **VisualizaciÃ³n de anomalÃ­as en los datos**  
- **Preprocesamiento, transformaciÃ³n e ingenierÃ­a de caracterÃ­sticas**  
- **Seguimiento de la procedencia del flujo de datos mediante metadatos de ML**  

## ğŸ“Œ TecnologÃ­as y Herramientas  

- **Docker** y **Docker Compose** para la configuraciÃ³n y aislamiento del entorno.  
- **Jupyter Notebook** como interfaz interactiva para la ejecuciÃ³n del cÃ³digo.  
- **Git y GitHub** para el control de versiones.  
- **Bash script y systemd** para la disponibilizaciÃ³n del ambiente cada vez que se inicie la VM.
- **Dataset**: *Tipo de Cubierta Forestal*  

| Column Name                                    | Variable Type  | Units / Range                        | Description                                      |
|------------------------------------------------|---------------|--------------------------------------|--------------------------------------------------|
| Elevation                                      | quantitative  | meters                               | Elevation in meters                             |
| Aspect                                         | quantitative  | azimuth                              | Aspect in degrees azimuth                       |
| Slope                                          | quantitative  | degrees                              | Slope in degrees                                |
| Horizontal_Distance_To_Hydrology               | quantitative  | meters                               | Horz Dist to nearest surface water features     |
| Vertical_Distance_To_Hydrology                 | quantitative  | meters                               | Vert Dist to nearest surface water features     |
| Horizontal_Distance_To_Roadways                | quantitative  | meters                               | Horz Dist to nearest roadway                   |
| Hillshade_9am                                  | quantitative  | 0 to 255 index                       | Hillshade index at 9am, summer solstice         |
| Hillshade_Noon                                 | quantitative  | 0 to 255 index                       | Hillshade index at noon, summer solstice        |
| Hillshade_3pm                                  | quantitative  | 0 to 255 index                       | Hillshade index at 3pm, summer solstice         |
| Horizontal_Distance_To_Fire_Points             | quantitative  | meters                               | Horz Dist to nearest wildfire ignition points   |
| Wilderness_Area (4 binary columns)            | qualitative   | 0 (absence) or 1 (presence)          | Wilderness area designation                     |
| Soil_Type (40 binary columns)                 | qualitative   | 0 (absence) or 1 (presence)          | Soil Type designation                          |
| Cover-Type (7 types)                           | integer       | 1 to 7                               | Forest Cover Type designation                   |

# Pipeline TFX para Tipo de Cobertura Forestal ![JupyterLab](https://jupyter.org/assets/homepage/main-logo.svg)

## DescripciÃ³n General
Este proyecto implementa un pipeline de TensorFlow Extended (TFX) para predecir tipos de cobertura forestal utilizando el conjunto de datos UCI Covertype. El pipeline abarca procesos de ingesta de datos, validaciÃ³n, transformaciÃ³n, ingenierÃ­a de caracterÃ­sticas y entrenamiento de modelos siguiendo las mejores prÃ¡cticas de MLOps.

## Requisitos Previos
- Python 3.7+
- TensorFlow 2.x
- TensorFlow Extended (TFX)
- TensorFlow Data Validation (TFDV)
- Pandas
- Scikit-learn
- ML Metadata

## Estructura del Proyecto
```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ covertype/         # Almacenamiento de datos sin procesar
â”‚   â”œâ”€â”€ data_new/          # Datos con selecciÃ³n de caracterÃ­sticas
â”‚   â””â”€â”€ data_service/      # Datos de prueba del entorno de servicio
â”œâ”€â”€ schema/                # Definiciones de esquema
â”œâ”€â”€ preprocessing.py       # MÃ³dulo de transformaciÃ³n de caracterÃ­sticas
â”œâ”€â”€ data_preparation.py    # Funciones de carga y preprocesamiento de datos
â”œâ”€â”€ model_creation.py      # Funciones de definiciÃ³n de modelos
â”œâ”€â”€ README.md              # Este archivo
```

## Componentes del Pipeline

1. **Ingesta de Datos**
   - Descarga el conjunto de datos Covertype desde UCI
   - Carga y prepara los datos para el procesamiento

2. **SelecciÃ³n de CaracterÃ­sticas**
   - Utiliza SelectKBest con f_classif para seleccionar las 8 caracterÃ­sticas mÃ¡s relevantes
   - Guarda el conjunto de datos reducido para su uso posterior

3. **GeneraciÃ³n de Ejemplos**
   - Utiliza CsvExampleGen para convertir los datos en formato TFRecord

4. **GeneraciÃ³n de EstadÃ­sticas**
   - Calcula estadÃ­sticas descriptivas para todas las caracterÃ­sticas
   - Visualiza las distribuciones de datos

5. **GeneraciÃ³n e Inferencia de Esquema**
   - Infiere automÃ¡ticamente un esquema a partir de los datos
   - Cura el esquema manualmente para definir dominios especÃ­ficos para caracterÃ­sticas crÃ­ticas
   - Define entornos de TRAINING y SERVING para gestionar diferencias entre ambos

6. **ValidaciÃ³n de Ejemplos**
   - Detecta anomalÃ­as en los datos respecto al esquema definido

7. **TransformaciÃ³n de CaracterÃ­sticas**
   - Implementa diferentes estrategias de escalado para las caracterÃ­sticas numÃ©ricas:
     - Escalado a rango [0, 1]
     - Escalado Min-Max
     - NormalizaciÃ³n Z-score

8. **InspecciÃ³n de Datos Transformados**
   - Obtiene ejemplos transformados y el grÃ¡fico de transformaciÃ³n
   - Lee y muestra ejemplos de TFRecord para verificar el formato de los datos transformados

9. **Metadatos de ML**
   - Almacena y rastrea todos los artefactos generados durante el pipeline
   - Permite la visualizaciÃ³n del linaje de datos
   - Incluye funciones auxiliares para:
     - Mostrar tipos de artefactos disponibles
     - Mostrar artefactos especÃ­ficos como esquemas
     - Mostrar propiedades de artefactos
     - Obtener artefactos principales que alimentaron otros artefactos

## Uso

Para ejecutar el pipeline completo:

```python
# Iniciar el contexto interactivo
context = InteractiveContext(pipeline_root=pipeline_root, 
                             metadata_connection_config=metadata_config)

# Ejecutar los componentes secuencialmente
context.run(example_gen)
context.run(statistics_gen)
context.run(schema_gen)
context.run(import_schema)
context.run(example_validator)
context.run(transform)

# Obtener los datos transformados y el grÃ¡fico de transformaciÃ³n
transformed_examples_channel = transform.outputs['transformed_examples']
transform_graph_channel = transform.outputs['transform_graph']

# Inspeccionar los datos transformados (ejemplos)
train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')
tfrecord_filenames = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]
dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type="GZIP")

# Verificar metadatos
store = mlmd.MetadataStore(metadata_config)
display_types(store.get_artifact_types())
schema_artifacts = store.get_artifacts_by_type("Schema")
display_artifacts(store, schema_artifacts)
```

## Funcionalidades de Metadatos

El pipeline incluye varias funciones auxiliares para trabajar con metadatos:

- `display_types()`: Muestra todos los tipos de artefactos disponibles en el almacÃ©n
- `display_artifacts()`: Muestra informaciÃ³n sobre artefactos especÃ­ficos
- `display_properties()`: Muestra propiedades detalladas de un artefacto
- `get_one_hop_parent_artifacts()`: Obtiene los artefactos padre de un artefacto determinado

Estas funciones facilitan el seguimiento del linaje de datos a lo largo del pipeline.

## Notas Adicionales

- El pipeline estÃ¡ diseÃ±ado para funcionar en modo interactivo para desarrollo y experimentaciÃ³n
- Los esquemas definidos consideran diferentes entornos (entrenamiento vs. servicio)
- Se implementan transformaciones de datos reutilizables a travÃ©s del mÃ³dulo `preprocessing.py`
- Se utiliza ML Metadata para el seguimiento de artefactos y linaje de datos

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n  mediante maquina virtual

### Prerrequisitos de la VM
AsegÃºrate de tener instalado en tu sistema:  
- [Docker](https://docs.docker.com/get-docker/)  
- [Docker Compose](https://docs.docker.com/compose/install/)  
- [Git](https://git-scm.com/)  

### Clonar el repositorio  
```bash
cd /home/estudiante/Desktop
git clone https://github.com/miguelhzuniga/mlops-repo.git
cd mlops-repo/proyecto-01/
```
### Crear y habilitar el servicio en VM

```bash
sudo cp jupyterlab.service /etc/systemd/system/jupyterlab.service 
sudo chmod +x jupyterlab.sh
sudo systemctl daemon-reload
sudo systemctl enable myscript.service
sudo systemctl start myscript.service
```
## ğŸ–¥ï¸ Instrucciones de uso
Iniciar la VM y loguearse.
```bash
ssh -L 8888:localhost:8888 estudiante@10.43.101.206
```
Abrir el entorno de desarrollo.
 - [Jupyterlab](http://localhost:8888/lab)  
