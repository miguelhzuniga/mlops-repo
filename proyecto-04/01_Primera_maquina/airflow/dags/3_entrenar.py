from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.dummy import DummyOperator
from datetime import datetime, timedelta
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
import joblib
import os
import mlflow
import mlflow.lightgbm

# Evidently API nueva
from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, TargetDriftPreset

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 5, 27),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False,
    'email_on_retry': False
}

dag = DAG(
    '3-Entrenar_modelo_con_drift_evidently',
    default_args=default_args,
    schedule_interval='5 0 * * *',
    catchup=False,
    max_active_runs=1,
    description='DAG para entrenar modelo con chequeo de data drift usando Evidently'
)

clean_schema = 'cleandata'
clean_table = 'processed_houses'
previous_data_path = '/tmp/previous_training_data.joblib'

def log_to_db(status, message, rmse=None):
    hook = PostgresHook(postgres_conn_id='postgres_default')
    sql = """
        INSERT INTO trainlogs.logs (status, message, rmse)
        VALUES (%s, %s, %s)
    """
    hook.run(sql, parameters=(status, message, rmse))

def check_data_count(**kwargs):
    hook = PostgresHook(postgres_conn_id='postgres_default')
    sql = f"SELECT COUNT(*) FROM {clean_schema}.{clean_table};"
    records = hook.get_first(sql)
    count = records[0] if records else 0
    print(f"Registros en tabla {clean_schema}.{clean_table}: {count}")
    if count > 20000:
        return "detect_data_drift_task"
    else:
        log_to_db("skip", "No se entren贸 el modelo: menos de 20,000 registros.")
        return "skip_training_task"

def detect_data_drift(**kwargs):
    hook = PostgresHook(postgres_conn_id='postgres_default')
    engine = hook.get_sqlalchemy_engine()
    query = f"SELECT * FROM {clean_schema}.{clean_table};"
    df_new = pd.read_sql(query, con=engine)

    relevant_cols = ['price', 'bed', 'bath', 'acre_lot']
    df_new = df_new[relevant_cols]

    categorical_cols = df_new.select_dtypes(include='object').columns.tolist()
    df_new[categorical_cols] = df_new[categorical_cols].astype('category')

    numerical_cols = [col for col in df_new.columns if col not in categorical_cols + ['price']]

    column_mapping = ColumnMapping(
        target='price',
        numerical_features=numerical_cols,
        categorical_features=categorical_cols
    )

    if not os.path.exists(previous_data_path):
        joblib.dump(df_new, previous_data_path)
        log_to_db("train", "No se detect贸 drift (primer entrenamiento).")
        return "train_model_task"

    df_old = joblib.load(previous_data_path)

    report = Report(metrics=[DataDriftPreset(), TargetDriftPreset()])
    report.run(reference_data=df_old, current_data=df_new, column_mapping=column_mapping)
    result = report.as_dict()

    drift_flag = result['metrics'][0]['result'].get('dataset_drift', False)
    target_drift_score = result['metrics'][1]['result'].get('drift_score', 0.0)

    if drift_flag or target_drift_score > 0.05:
        log_to_db("skip", f"Drift detectado: dataset_drift={drift_flag}, target_drift_score={target_drift_score:.4f}")
        return "skip_training_task"
    else:
        joblib.dump(df_new, previous_data_path)
        log_to_db("train", "No se detect贸 drift, se procede a entrenar el modelo.")
        return "train_model_task"

def train_model(**kwargs):
    hook = PostgresHook(postgres_conn_id='postgres_default')
    engine = hook.get_sqlalchemy_engine()
    query = f"SELECT * FROM {clean_schema}.{clean_table};"
    df = pd.read_sql(query, con=engine)

    y = df['price']
    X = df.drop(columns=['id', 'price', 'prev_sold_date'], errors='ignore')
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype('category')

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    df_train = X_train.copy()
    df_train['price'] = y_train
    joblib.dump(df_train, previous_data_path)
    print(f"Datos de entrenamiento guardados en {previous_data_path}")

    model = LGBMRegressor(objective='regression', n_estimators=50, random_state=42)

    mlflow.set_tracking_uri("http://mlflow.mlops-project.svc.cluster.local:5000")
    mlflow.set_experiment("lightgbm_housing")

    with mlflow.start_run():
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)])

        y_pred = model.predict(X_val, num_iteration=model.best_iteration_)
        rmse = mean_squared_error(y_val, y_pred, squared=False)
        print(f"RMSE validaci贸n: {rmse:.4f}")

        mlflow.log_param("n_estimators", 50)
        mlflow.log_metric("rmse", rmse)
        mlflow.lightgbm.log_model(model, artifact_path="model")

        log_to_db("train", "Modelo entrenado y registrado en MLflow.", rmse=rmse)

    print("Modelo guardado y logueado en MLflow.")

with dag:

    check_data_task = BranchPythonOperator(
        task_id='check_data_count',
        python_callable=check_data_count
    )

    detect_data_drift_task = BranchPythonOperator(
        task_id='detect_data_drift_task',
        python_callable=detect_data_drift
    )

    train_model_task = PythonOperator(
        task_id='train_model_task',
        python_callable=train_model
    )

    skip_training_task = DummyOperator(
        task_id='skip_training_task'
    )

    end_task = DummyOperator(
        task_id='end_task',
        trigger_rule='none_failed_min_one_success'
    )

    check_data_task >> detect_data_drift_task >> [train_model_task, skip_training_task] >> end_task
