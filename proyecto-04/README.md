# Proyecto Final - Operaciones de Machine Learning
## Sistema MLOps para PredicciÃ³n de Precios de Bienes RaÃ­ces

### Video del proyecto

A continuaciÃ³n puede acceder al video explicativo del proyecto:

[Proyecto final MLOPS](https://youtu.be/gQF0ej60K2c)

### ğŸ¯ DescripciÃ³n General

Este proyecto implementa una plataforma MLOps empresarial completa para la predicciÃ³n automatizada de precios de propiedades inmobiliarias. El sistema integra las mejores prÃ¡cticas de DevOps, MLOps y observabilidad, proporcionando un flujo de trabajo automatizado desde la recolecciÃ³n de datos hasta el despliegue en producciÃ³n con CI/CD completo.

### ğŸ—ï¸ Arquitectura del Sistema

La soluciÃ³n estÃ¡ diseÃ±ada con una arquitectura distribuida de microservicios desplegada en tres mÃ¡quinas virtuales especializadas:

- **ğŸ”„ MÃ¡quina 01 - Data & ML Pipeline**: OrquestaciÃ³n con Airflow y registro de experimentos con MLflow
- **ğŸš€ MÃ¡quina 02 - Servicios de Inferencia**: API REST (FastAPI) e interfaz de usuario (Gradio)
- **ğŸ“Š MÃ¡quina 03 - GitOps & Observabilidad**: Despliegue continuo (Argo CD) y monitorizaciÃ³n (Prometheus + Grafana)


### ğŸ“¦ **Workflow: "Construir y Subir ImÃ¡genes MLOps"**

El pipeline automatiza completamente el ciclo de vida del software con 6 etapas principales:

#### **1. ğŸ—ï¸ Builds Paralelos de ImÃ¡genes**
```yaml
jobs:
  build-airflow:    # ğŸ”„ Imagen Airflow
  build-mlflow:     # ğŸ“Š Imagen MLflow  
  build-fastapi:    # âš¡ Imagen FastAPI
  build-gradio:     # ğŸ¨ Imagen Gradio
```

**CaracterÃ­sticas**:
- âœ… Builds paralelos para mÃ¡xima eficiencia
- ğŸ·ï¸ Tagging automÃ¡tico por rama y SHA
- ğŸŒ Multi-arquitectura (AMD64 + ARM64)
- ğŸ“¦ CachÃ© optimizado GitHub Actions
- ğŸ³ Push automÃ¡tico a Docker Hub

#### **2. ğŸ·ï¸ Sistema de Tagging Inteligente**
```bash
# Rama master (producciÃ³n)
Tag: YYYYMMDD-{short-sha}    # Ej: 20250603-a1b2c3d

# Ramas de desarrollo  
Tag: {branch-name}           # Ej: feature-new-model
```

#### **3. ğŸ“ ActualizaciÃ³n AutomÃ¡tica de Manifiestos**
```yaml
actualizar-manifiestos:
  needs: [build-airflow, build-mlflow, build-fastapi, build-gradio]
  if: github.ref == 'refs/heads/master'
```

**Proceso**:
1. ğŸ” Detecta nuevos tags de imagen
2. ğŸ“ Actualiza manifiestos Kubernetes automÃ¡ticamente
3. ğŸ’¾ Commit y push de cambios
4. ğŸ”„ Dispara sincronizaciÃ³n de Argo CD

#### **4. ğŸ¯ DistribuciÃ³n por MÃ¡quina**

| MÃ¡quina | Componente | Despliegue | Manifiesto |
|---------|------------|------------|------------|
| 01 | Airflow | Docker Compose | Manual |
| 01 | MLflow | Argo CD | `mlflow/manifests/mlflow.yaml` |
| 02 | FastAPI | Argo CD | `fastapi/fastapi-deployment.yaml` |
| 02 | Gradio | Argo CD | `gradio/gradio-deployment.yaml` |

#### **5. âš¡ Triggers del Pipeline**

**Push a Master** (ProducciÃ³n):
```yaml
on:
  push:
    branches: [ master ]
    paths:
      - 'proyecto-04/01_Primera_maquina/airflow/**'
      - 'proyecto-04/01_Primera_maquina/mlflow/**'  
      - 'proyecto-04/02_Segunda_maquina/api/**'
```

**Pull Requests** (Testing):
```yaml
on:
  pull_request:
    branches: [ master ]
```

#### **6. ğŸ“Š MonitorizaciÃ³n y Reportes**
```bash
âœ… Pipeline Status:
â€¢ Build Airflow:          success
â€¢ Build MLflow:           success  
â€¢ Build FastAPI:          success
â€¢ Build Gradio:           success
â€¢ Actualizar Manifiestos: success

ğŸ“¦ ImÃ¡genes en Docker Hub:
â€¢ username/airflow-houses:20250603-a1b2c3d  (Ejemplo)
â€¢ username/mlflow-houses:20250603-a1b2c3d   (Ejemplo)
â€¢ username/fastapi-houses:20250603-a1b2c3d  (Ejemplo)
â€¢ username/gradio-houses:20250603-a1b2c3d   (Ejemplo)
```

---

## ğŸ” ConfiguraciÃ³n de Secrets

### **GitHub Secrets Requeridos**:
```bash
DOCKER_USERNAME= Usuario-dockerhub
DOCKER_PASSWORD= Token-dockerhub
GITHUB_TOKEN= Auto-generado
```

### **Permisos GitHub Actions**:
```yaml
permissions:
  contents: write    # Para commits automÃ¡ticos
  packages: write    # Para Docker registry
  actions: read      # Para workflows
```

---

## ğŸš€ Flujo de Despliegue Completo

### **1. ğŸ‘¨â€ğŸ’» Desarrollo**
```bash
# Developer pushea cÃ³digo
git push origin feature-nueva-funcionalidad
```

### **2. ğŸ”„ CI/CD Pipeline**
![alt text](./Imagenes/image-2.png)

### **3. ğŸ“¦ GitOps con Argo CD**
```bash
# Argo CD detecta cambios (3 min)
Sync Status: OutOfSync â†’ Syncing â†’ Healthy

# Aplicaciones monitoreadas:
â€¢ mlflow-app      (MÃ¡quina 01)
â€¢ fastapi-app     (MÃ¡quina 02)  
â€¢ gradio-app      (MÃ¡quina 02)
```

### **4. ğŸ¯ Despliegue por MÃ¡quina**

#### **MÃ¡quina 01 - AutomÃ¡tico/Manual**:
```bash
# MLflow (AutomÃ¡tico vÃ­a Argo CD)
kubectl set image deployment/mlflow mlflow=user/mlflow-houses:new-tag

# Airflow (Manual - Docker Compose)
cd airflow && docker-compose pull && docker-compose up -d
```

#### **MÃ¡quina 02 - AutomÃ¡tico**:
```bash
# FastAPI (AutomÃ¡tico vÃ­a Argo CD)
kubectl set image deployment/fastapi-housing fastapi=user/fastapi-houses:new-tag

# Gradio (AutomÃ¡tico vÃ­a Argo CD)  
kubectl set image deployment/gradio-housing gradio=user/gradio-houses:new-tag
```

#### **MÃ¡quina 03 - GestiÃ³n**:
```bash
# Monitoreo en tiempo real
Argo CD:    https://localhost:30443
Grafana:    https://localhost:30000
Prometheus: https://localhost:30090
```

---


### ğŸ“Š Dataset y Fuente de Datos

Los datos provienen de **Realtor.com**, el segundo sitio web de bienes raÃ­ces mÃ¡s visitado de Estados Unidos, con mÃ¡s de 100 millones de usuarios activos mensuales. La informaciÃ³n se obtiene de una API externa que simula la llegada incremental de datos en un entorno productivo.

#### Variables del Dataset

| Variable | Tipo | DescripciÃ³n |
|----------|------|-------------|
| `brokered_by` | CategÃ³rica | Agencia/corredor inmobiliario |
| `status` | CategÃ³rica | Estado: lista para venta/construcciÃ³n |
| `price` | NumÃ©rica | Precio objetivo (variable a predecir) |
| `bed` | NumÃ©rica | NÃºmero de habitaciones |
| `bath` | NumÃ©rica | NÃºmero de baÃ±os |
| `acre_lot` | NumÃ©rica | TamaÃ±o del terreno en acres |
| `street` | CategÃ³rica | DirecciÃ³n (codificada) |
| `city` | CategÃ³rica | Ciudad |
| `state` | CategÃ³rica | Estado |
| `zip_code` | CategÃ³rica | CÃ³digo postal |
| `house_size` | NumÃ©rica | Ãrea habitable en pies cuadrados |
| `prev_sold_date` | Temporal | Fecha de venta anterior |

**ğŸ¯ Objetivo**: Predecir el precio de una propiedad basÃ¡ndose en sus caracterÃ­sticas fÃ­sicas y de ubicaciÃ³n.

### ğŸ“ Estructura del Proyecto

```
proyecto-04/
â”œâ”€â”€ 01_Primera_maquina/
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”œâ”€â”€ 0_borrar.py
â”‚   â”‚   â”‚   â”œâ”€â”€ 1_recopilar.py
â”‚   â”‚   â”‚   â”œâ”€â”€ 2_procesar.py
â”‚   â”‚   â”‚   â”œâ”€â”€ 3_entrenar.py
â”‚   â”‚   â”‚   â””â”€â”€ entrenar modificado.txt
â”‚   â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ mlflow/
â”‚       â”œâ”€â”€ Dockerfile.mlflow
â”‚       â”œâ”€â”€ cleanup.sh
â”‚       â”œâ”€â”€ deploy.sh
â”‚       â”œâ”€â”€ docker/
â”‚       â”‚   â””â”€â”€ Dockerfile.mlflow
â”‚       â””â”€â”€ manifests/
â”‚           â”œâ”€â”€ ingress.yaml
â”‚           â”œâ”€â”€ init-job.yaml
â”‚           â”œâ”€â”€ mlflow.yaml
â”‚           â”œâ”€â”€ namespace.yaml
â”‚           â””â”€â”€ storage.yaml
â”œâ”€â”€ 02_Segunda_maquina/
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ clean_api.sh
â”‚       â”œâ”€â”€ deploy.sh
â”‚       â”œâ”€â”€ fastapi/
â”‚       â”‚   â”œâ”€â”€ Dockerfile
â”‚       â”‚   â”œâ”€â”€ fastapi-deployment.yaml
â”‚       â”‚   â”œâ”€â”€ fastapi-service.yaml
â”‚       â”‚   â”œâ”€â”€ main_server.py
â”‚       â”‚   â””â”€â”€ requirements.txt
â”‚       â”œâ”€â”€ gradio/
â”‚       â”‚   â”œâ”€â”€ Dockerfile
â”‚       â”‚   â”œâ”€â”€ gradio-deployment.yaml
â”‚       â”‚   â”œâ”€â”€ gradio-service.yaml
â”‚       â”‚   â”œâ”€â”€ gradio_app.py
â”‚       â”‚   â””â”€â”€ requirements.txt
â”‚       â””â”€â”€ gradio modificacion.txt
â”œâ”€â”€ 03_Tercera_maquina/
â”‚   â”œâ”€â”€ argo-cd/
â”‚   â”‚   â”œâ”€â”€ app.yaml
â”‚   â”‚   â”œâ”€â”€ apps/
â”‚   â”‚   â”‚   â”œâ”€â”€ fastapi.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ gradio.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ mlflow.yaml
â”‚   â”‚   â”‚   â””â”€â”€ monitoring.yaml
â”‚   â”‚   â””â”€â”€ install.yaml
â”‚   â”œâ”€â”€ cleanall.sh
â”‚   â”œâ”€â”€ deploy-argo.sh
â”‚   â”œâ”€â”€ deploy-monitoring.sh
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ grafana.yaml
â”‚   â”‚   â””â”€â”€ prometheus.yaml
â”‚   â””â”€â”€ stop_port_forwards.sh
â””â”€â”€ README.md

# GitHub Actions (fuera de proyecto-04)
.github/workflows/
â””â”€â”€ mlops-proyecto-final-ci.yml

```
## ğŸ‘¨â€ğŸ’» ConfiguraciÃ³n del Cluster Microk8s
Se implementÃ³ una configuraciÃ³n que distribuye la carga del proyecto entre tres mÃ¡quinas mediante Kubernetes, utilizando MicroK8s en un Ãºnico clÃºster. Estas mÃ¡quinas comparten recursos de forma coordinada, lo que permite aprovechar de manera eficiente sus capacidades conjuntas. Como resultado, se obtiene lo siguiente:

---
![alt text](./Imagenes/kubernete.png)
---


## ğŸ”„ MÃ¡quina 01 - Data Pipeline & ML Operations

### DescripciÃ³n

Esta mÃ¡quina forma el nÃºcleo del pipeline de datos y experimentaciÃ³n, integrando **Apache Airflow** para orquestaciÃ³n y **MLflow** para gestiÃ³n del ciclo de vida de modelos.

### Componentes Principales

## Apache Airflow - OrquestaciÃ³n (Puerto 8080)

![alt text](./Imagenes/airflow.png)

Gestiona el flujo completo de datos mediante 4 DAGs especializados:

**1. `0_borrar.py` - Reset de Esquemas**
- Elimina los esquemas `rawdata`, `cleandata` y `trainlogs` con CASCADE
- Verifica esquemas disponibles antes de la eliminaciÃ³n
- Garantiza un estado limpio para cada ejecuciÃ³n del pipeline

**2. `1_recopilar.py` - RecolecciÃ³n de Datos**
- Consulta la API externa (`http://10.43.101.108:80/data`)
- Implementa lÃ³gica de reinicio automÃ¡tico cuando se agota la data disponible
- Almacena datos crudos en PostgreSQL (`rawdata.houses`)
- Retry logic robusto con manejo de errores y reintentos automÃ¡ticos

**3. `2_procesar.py` - Pipeline ETL**
- Limpieza de datos: elimina registros con precio â‰¤ 0 o tamaÃ±o â‰¤ 0
- IngenierÃ­a de caracterÃ­sticas: calcula `price_per_sqft`
- NormalizaciÃ³n de campos categÃ³ricos (status a minÃºsculas)
- Almacenamiento en `cleandata.processed_houses`

**4. `3_entrenar.py` - Entrenamiento ML con Drift Detection**
- ValidaciÃ³n automÃ¡tica de volumen mÃ­nimo de datos (20,000+ registros)
- **DetecciÃ³n de data drift** usando Evidently (DataDriftPreset y TargetDriftPreset)
- Entrenamiento condicional: solo si detecta drift o es primera ejecuciÃ³n
- Benchmarking de algoritmos: **LightGBMRegressor** y **DecisionTreeRegressor**
- Preprocessado automÃ¡tico con StandardScaler y OneHotEncoder
- Registro en MLflow con promociÃ³n automÃ¡tica del mejor modelo a "Production"
- Logging detallado en `trainlogs.logs` con mÃ©tricas de rendimiento

## MLflow - ML Lifecycle Management (Puerto 30500)

![alt text](./Imagenes/mlflow1.png)
---
![alt text](./Imagenes/mlflow2.png)
---

**Arquitectura de Componentes:**
- **Tracking Server**: MLflow v2.10.0 con 3 rÃ©plicas para alta disponibilidad
- **Model Registry**: Versionado automÃ¡tico y gestiÃ³n de transiciones de modelos
- **Backend Store**: PostgreSQL (puerto interno) para metadatos de experimentos
- **Artifact Store**: MinIO S3-compatible (API: 30382, Console: 30901) para modelos y artefactos
---
![alt text](./Imagenes/minio.png)
---

**Infraestructura Kubernetes:**
- **Namespace**: `mlops-project` dedicado para aislamiento
- **Persistencia**: PVCs con `microk8s-hostpath` (PostgreSQL: 5Gi, MinIO: 10Gi)
- **Networking**: Ingress para routing inteligente + NodePort para acceso directo
- **Health Checks**: Liveness y readiness probes en `/health` endpoint
- **Recursos**: Requests optimizados (512Mi RAM, 200m CPU) con lÃ­mites escalables

**Funcionalidades Implementadas:**
- Tracking automÃ¡tico de hiperparÃ¡metros, mÃ©tricas y artefactos
- GestiÃ³n de stages: `None` â†’ `Staging` â†’ `Production` â†’ `Archived`
- ComparaciÃ³n visual de experimentos y linaje de modelos
- IntegraciÃ³n S3 nativa con bucket `mlflow-artifacts` preconfigurado

**Credenciales de Acceso:**
- **MinIO Console**: `adminuser` / `securepassword123`
- **MLflow**: Sin autenticaciÃ³n (acceso directo via NodePort)

## Especificaciones TÃ©cnicas

#### Infraestructura
- **Airflow**: Docker Compose con Apache Airflow 2.10.5 (Python 3.8)
- **MLflow**: Kubernetes con MLflow 2.10.0 + PostgreSQL backend
- **Executor**: CeleryExecutor con Redis 7.2-bookworm como broker
- **Base de Datos**: PostgreSQL 13 con schemas separados (`rawdata`, `cleandata`, `trainlogs`)
- **Almacenamiento**: MinIO S3-compatible con PVCs (PostgreSQL: 5Gi, MinIO: 10Gi)
- **Recursos**: 4GB RAM, 2 CPUs mÃ­nimo recomendado

#### Arquitectura de Servicios
- **Airflow Components**: Webserver, Scheduler, Worker, Triggerer + Init
- **MLflow Components**: Tracking Server (3 rÃ©plicas), Model Registry, Artifact Store
- **Monitoring**: **PgAdmin4** para gestiÃ³n de base de datos
---
![alt text](./Imagenes/pgadmin.png)

* Como se observa en la anterior imagen, la columna "data_origin" contiene la etiqueta que indica el origen de los datos. Si el dato se recolecto desde la API del profesor queda con la etiqueta "teacher", mientras que si se recolecta desde la interfaz de usuario de GRADIO, se guarda la etiqueta "user". Siempre que el usuario genere una nueva prediccion en gradio se agrega una nueva fila con la data correspondiente a esta para reentrenar el modelo. El modelo solo se reentrena si los datos contienen el 80% de datos con etiqueta "teacher". 


---
- **Storage**: VolÃºmenes persistentes para DAGs, logs, models, plugins

#### ConfiguraciÃ³n de Red
| Servicio | Puerto | Acceso | Credenciales |
|----------|--------|--------|--------------|
| Airflow Webserver | 8080 | `http://10.43.101.175:8080` | `airflow/airflow` |
| MLflow Tracking | 30500 | `http://10.43.101.175:30500` | Sin autenticaciÃ³n |
| PostgreSQL | 5432 | Interno (Docker/K8s network) | `airflow/airflow` |
| PgAdmin | 5050 | `http://10.43.101.175:5050` | `admin@example.com/admin` |
| MinIO Console | 30901 | `http://10.43.101.175:30901` | `adminuser/securepassword123` |
| MinIO API | 30382 | `http://10.43.101.175:30382` | S3-compatible |
| Redis | 6379 | Interno (Celery backend) | Sin autenticaciÃ³n |

#### Variables de Entorno Clave
```bash
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__CORE__DEFAULT_TIMEZONE=America/Bogota
MLFLOW_TRACKING_URI=http://10.43.101.175:30500
MLFLOW_S3_ENDPOINT_URL=http://10.43.101.175:30382
HOST_IP=10.43.101.175
```

### Instrucciones de Despliegue Inicial

```bash
# 1. Desplegar Airflow (Docker Compose)
cd 01_Primera_maquina/airflow
chmod +x deploy.sh
./deploy.sh

# 2. Desplegar MLflow (Kubernetes)
cd ../mlflow
chmod +x deploy.sh
./deploy.sh

# 3. Verificar despliegue
docker-compose ps                    # Verificar Airflow
kubectl get pods -n mlops-project   # Verificar MLflow
```

---

## ğŸš€ MÃ¡quina 02 - Servicios de Inferencia

Esta mÃ¡quina implementa la capa de inferencia del sistema, proporcionando tanto una API REST para integraciÃ³n programÃ¡tica como una interfaz web interactiva para usuarios finales usando FastAPI y Gradio.

## Arquitectura de Microservicios

### FastAPI - Backend de Inferencia (Puerto 30601)

---
![alt text](./Imagenes/fastapi.png)
---

**CaracterÃ­sticas TÃ©cnicas:**
- **Imagen**: `luisfrontuso10/fastapi-houses:20250603-afef697` (Imagen que cambia conforme el proceso de CI/CD )
- **Replicas**: 3 instancias para alta disponibilidad
- **ConexiÃ³n dinÃ¡mica a MLflow**: Carga automÃ¡tica del modelo marcado como "Production"
- **Preprocesamiento**: Carga automÃ¡tica del preprocesador desde MinIO S3
- **ValidaciÃ³n de datos**: Pydantic schemas con HouseFeatures model
- **InstrumentaciÃ³n**: MÃ©tricas Prometheus integradas (requests, predictions, errors, timing)

**Endpoints Principales:**
```python
GET  /                    # InformaciÃ³n de la API
POST /predict            # PredicciÃ³n de precios
GET  /health             # Health check (modelo + preprocesador)
GET  /metrics            # MÃ©tricas Prometheus
POST /test_preprocess    # Test de preprocesamiento
```

**Ejemplo de Request:**
```json
{
  "brokered_by": "101640.0",
  "status": "for_sale", 
  "bed": 3,
  "bath": 2,
  "acre_lot": 0.25,
  "street": "1758218.0",
  "city": "East Windsor",
  "state": "Connecticut", 
  "zip_code": "6016.0",
  "house_size": 1500,
  "prev_sold_date": "2020-01-01"
}
```

### Gradio - Frontend Interactivo (Puerto 30602)

---
## PANEL DE PREDICCIÃ“N
![alt text](./Imagenes/gradio1.png)
---
## PANEL DE LOGS DE ENTRENAMIENTO
![alt text](./Imagenes/gradio2.png)
---
## PANEL DE LOGS DE ANALISIS SHAP
![alt text](./Imagenes/gradio3.png)
---

**CaracterÃ­sticas TÃ©cnicas:**
- **Imagen**: `luisfrontuso10/gradio-houses:20250603-afef697` (Imagen que cambia conforme el proceso de CI/CD )
- **Replicas**: 3 instancias con load balancing
- **Servidor de mÃ©tricas**: FastAPI integrado en puerto 9090 para Prometheus

**Funcionalidades Implementadas:**

**1. PestaÃ±a de PredicciÃ³n:**
- Selector dinÃ¡mico de modelos MLflow con refresh automÃ¡tico
- Formularios organizados por categorÃ­as (ubicaciÃ³n, caracterÃ­sticas, fechas)
- Carga de modelos bajo demanda desde Model Registry
- Resultados formateados con precio estimado en USD

**2. PestaÃ±a de Logs (trainlogs.logs):**
- ConexiÃ³n directa a PostgreSQL para consultar logs de entrenamiento
- VisualizaciÃ³n de tabla con Ãºltimos registros de `trainlogs.logs`
- InformaciÃ³n de estado, mensajes y mÃ©tricas RMSE

**3. AnÃ¡lisis SHAP con Nombres Descriptivos:**
- **TreeExplainer** para modelos LightGBM con extracciÃ³n automÃ¡tica del modelo subyacente
- **Mapeo inteligente** de caracterÃ­sticas: `num__bed` â†’ `Habitaciones`, `cat__status__for_sale` â†’ `Estado: En venta`
- **Top 15 caracterÃ­sticas** mÃ¡s importantes con nombres legibles
- **MÃ©todo hÃ­brido de backup** con anÃ¡lisis de sensibilidad por caracterÃ­stica
- Visualizaciones: Summary plots, Feature importance, Impact analysis

## Flujo de Trabajo de Inferencia
![alt text](./Imagenes/image.png)

## Especificaciones TÃ©cnicas

### Infraestructura Kubernetes
- **Namespace**: `mlops-project`
- **Deployment strategy**: Rolling updates con 3 rÃ©plicas por servicio
- **Resource allocation**: 
  - **Requests**: 512Mi RAM, 200m CPU por replica
  - **Limits**: 1Gi RAM, 500m CPU por replica
- **Health monitoring**: Liveness (60s) y readiness (30s) probes configurados

### ConfiguraciÃ³n de Red
| Servicio | Puerto Interno | NodePort | Acceso |
|----------|----------------|----------|--------|
| FastAPI Backend | 80 | 30601 | `http://10.43.101.175:30601` |
| Gradio Frontend | 8501 | 30602 | `http://10.43.101.175:30602` |
| Prometheus Metrics | 9090 | - | Interno (scraping) |

### Variables de Entorno
```bash
MLFLOW_TRACKING_URI=http://10.43.101.175:30500
MLFLOW_S3_ENDPOINT_URL=http://10.43.101.175:30382
AWS_ACCESS_KEY_ID=adminuser
AWS_SECRET_ACCESS_KEY=securepassword123
```

## CaracterÃ­sticas Avanzadas

### Robustez y Confiabilidad
- **Health checks**: Liveness y readiness probes configurados (60s/30s intervals)
- **Resource management**: Requests (512Mi RAM, 200m CPU) y limits (1Gi RAM, 500m CPU)
- **Caching inteligente**: Cache en memoria para modelos y preprocesadores
- **Manejo de errores**: Try-catch con fallbacks para anÃ¡lisis SHAP
- **Graceful shutdown**: Manejo adecuado de seÃ±ales de terminaciÃ³n

### Conectividad y ConfiguraciÃ³n
- **Variables de entorno**: MLflow URI, MinIO S3, credenciales AWS
- **ConexiÃ³n a servicios**: PostgreSQL para logs, MLflow para modelos, MinIO para artefactos
- **CORS**: ConfiguraciÃ³n permisiva para desarrollo (`allow_origins=["*"]`)
- **SerializaciÃ³n**: Soporte para joblib y dill para compatibilidad de artefactos

### MonitorizaciÃ³n y Observabilidad
**MÃ©tricas Prometheus FastAPI:**
- `house_api_requests_total` - Total de solicitudes
- `house_api_predictions_total` - Total de predicciones exitosas
- `house_api_prediction_time_seconds` - Tiempo de procesamiento
- `house_api_model_errors_total` - Errores del modelo

**MÃ©tricas Prometheus Gradio:**
- `house_price_gradio_requests_total` - Solicitudes a la interfaz
- `house_price_gradio_predictions_total` - Predicciones desde UI
- `house_price_gradio_model_loads_total` - Cargas de modelo
- `house_price_gradio_refresh_calls_total` - Actualizaciones de modelos

### Interpretabilidad y Explicabilidad

**SHAP Analysis Engine:**
```python
# Mapeo inteligente de caracterÃ­sticas
important_patterns = {
    'bed': 'Habitaciones',
    'bath': 'BaÃ±os', 
    'acre_lot': 'Terreno (acres)',
    'house_size': 'TamaÃ±o casa (sqft)',
    'prev_sold_year': 'AÃ±o venta anterior',
    'status__for_sale': 'Estado: En venta',
    'state__Connecticut': 'Estado: Connecticut',
    'city__East Windsor': 'Ciudad: East Windsor'
}
```

**AnÃ¡lisis HÃ­brido de Backup:**
- AnÃ¡lisis de sensibilidad por caracterÃ­stica
- VisualizaciÃ³n de impacto en precio base
- ComparaciÃ³n con diferentes configuraciones de propiedades

## Instrucciones de Despliegue

### Despliegue AutomÃ¡tico Antes de CI/CD
```bash
cd prediction-api
chmod +x deploy.sh
./deploy.sh
```

### VerificaciÃ³n del Despliegue
```bash
# Verificar pods
microk8s kubectl get pods -n mlops-project | grep -E 'fastapi|gradio'

# Verificar servicios
microk8s kubectl get services -n mlops-project | grep -E 'fastapi|gradio'

# Verificar logs
microk8s kubectl logs -f deployment/fastapi-housing -n mlops-project
microk8s kubectl logs -f deployment/gradio-housing -n mlops-project
```

### Acceso a Servicios
```bash
echo "FastAPI Backend: http://10.43.101.175:30601"
echo "FastAPI Docs: http://10.43.101.175:30601/docs"
echo "Gradio UI: http://10.43.101.175:30602"
echo "Health Check: http://10.43.101.175:30601/health"
echo "Metrics: http://10.43.101.175:30601/metrics"
```

### Cleanup y Mantenimiento
```bash
# Eliminar servicios
chmod +x clean_api.sh
./clean_api.sh

# Restart servicios
microk8s kubectl rollout restart deployment/fastapi-housing -n mlops-project
microk8s kubectl rollout restart deployment/gradio-housing -n mlops-project
```

## Dependencias y Requisitos

### FastAPI Requirements
```txt
fastapi==0.110.0
uvicorn==0.27.0.post1
mlflow==2.3.0
numpy==1.21.0
pandas==1.3.3
scikit-learn==1.0.2
prometheus_client
boto3
pydantic
dill==0.3.6
lightgbm==3.3.2
```

### Gradio Requirements
```txt
fastapi==0.95.0
uvicorn==0.27.0.post1
mlflow==2.10.0
numpy==1.22.0
pandas==1.5.3
scikit-learn==1.0.2
scipy==1.10.1
lightgbm==3.3.2
shap==0.44.0
matplotlib==3.7.1
gradio==3.50.2
requests==2.28.2
boto3==1.26.121
dill==0.3.6
joblib==1.2.0
psycopg2-binary==2.9.9
prometheus_client==0.14.1
```

## Estructura de Archivos

```
prediction-api/
â”œâ”€â”€ fastapi/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main_server.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ fastapi-deployment.yaml
â”‚   â””â”€â”€ fastapi-service.yaml
â”œâ”€â”€ gradio/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ gradio_app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ gradio-deployment.yaml
â”‚   â””â”€â”€ gradio-service.yaml
â”œâ”€â”€ deploy.sh
â””â”€â”€ clean_api.sh
```

## ğŸ“Š MÃ¡quina 03 - GitOps & Observabilidad

## DescripciÃ³n
Esta mÃ¡quina implementa las capacidades de despliegue continuo y observabilidad del sistema, utilizando **Argo CD** para GitOps y un stack completo de monitorizaciÃ³n con **Prometheus** y **Grafana**.

## Argo CD - Despliegue Continuo GitOps
![alt text](./Imagenes/argocd.png)
---


### ConfiguraciÃ³n de Acceso
- **NodePort HTTP**: Puerto 30080 (`http://localhost:30080`)
- **NodePort HTTPS**: Puerto 30443 (`https://localhost:30443`)  
- **Port-forwarding**: Puerto 8081 (`https://localhost:8081`)
- **Credenciales**: `admin` / `<contraseÃ±a-generada-automÃ¡ticamente>`

### Arquitectura "App of Apps"
El sistema utiliza el patrÃ³n "App of Apps" para gestiÃ³n centralizada:

```yaml
# mlops-root-app - AplicaciÃ³n RaÃ­z
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: mlops-root-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/miguelhzuniga/mlops-repo.git
    targetRevision: master
    path: proyecto-04/03_Tercera_maquina/argo-cd/apps
    directory:
      recurse: true
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true      # Elimina recursos obsoletos
      selfHeal: true   # Auto-correcciÃ³n de drift
```

### Aplicaciones Gestionadas
| AplicaciÃ³n | Ruta del Repositorio | Namespace | DescripciÃ³n |
|------------|---------------------|-----------|-------------|
| `mlflow-app` | `proyecto-04/01_Primera_maquina/mlflow/manifests` | `mlops-project` | MLflow Tracking Server |
| `fastapi-app` | `proyecto-04/02_Segunda_maquina/api/fastapi` | `mlops-project` | API de inferencia |
| `gradio-app` | `proyecto-04/02_Segunda_maquina/api/gradio` | `mlops-project` | Interfaz web interactiva |
| `monitoring-app` | `proyecto-04/03_Tercera_maquina/monitoring` | `mlops-project` | Stack de observabilidad |

### CaracterÃ­sticas GitOps
- **Declarativo**: Estado deseado definido en Git
- **Observabilidad**: Monitoreo continuo de drift
- **AutomatizaciÃ³n**: SincronizaciÃ³n automÃ¡tica con `prune` y `selfHeal`
- **Rollback**: ReversiÃ³n automÃ¡tica en caso de fallos
- **Multi-aplicaciÃ³n**: GestiÃ³n centralizada de todo el stack MLOps

## Stack de Observabilidad

### Prometheus - RecolecciÃ³n de MÃ©tricas (Puerto 31090)
![alt text](./Imagenes/prometheus.png)
---

**ConfiguraciÃ³n de Scraping:**
```yaml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'fastapi'
    static_configs:
      - targets: ['10.43.101.202:30601']
  - job_name: 'gradio'
    static_configs:
      - targets: ['10.43.101.202:30601']
```

### Grafana - VisualizaciÃ³n (Puerto 31300)
![alt text](./Imagenes/grafana.png)
---

- **Credenciales**: `admin` / `admin`
- **ConfiguraciÃ³n**: Deployment bÃ¡sico con ConfigMap

**Dashboards Potenciales:**

**1. MLOps Overview Dashboard:**
- Rate de predicciones por minuto/hora
- Latencia de API (percentiles P50, P95, P99)
- Tasa de errores y disponibilidad
- Cargas de modelos y refreshes

**2. Model Performance Dashboard:**
- DistribuciÃ³n de predicciones
- AnÃ¡lisis SHAP ejecutados
- ComparaciÃ³n entre modelos
- Logs de entrenamiento (trainlogs.logs)

**3. Infrastructure Dashboard:**
- Estado de pods FastAPI y Gradio (3 rÃ©plicas cada uno)
- UtilizaciÃ³n de recursos (CPU: 200m-500m, RAM: 512Mi-1Gi)
- Health checks y readiness probes
- Estado de servicios MLflow y MinIO

## ConfiguraciÃ³n de Red y Puertos

| Servicio | Puerto Interno | NodePort | Acceso |
|----------|----------------|----------|--------|
| Argo CD HTTP | 8080 | 30080 | `http://localhost:30080` |
| Argo CD HTTPS | 8080 | 30443 | `https://localhost:30443` |
| Prometheus | 9090 | 31090 | `http://<NODE_IP>:31090` |
| Grafana | 3000 | 31300 | `http://<NODE_IP>:31300` |

## Instrucciones de Despliegue

### Despliegue de Argo CD
```bash
cd 03_Tercera_maquina

# Desplegar Argo CD con configuraciÃ³n automÃ¡tica
chmod +x deploy-argo.sh
sudo ./deploy-argo.sh

# El script preguntarÃ¡ el mÃ©todo de acceso preferido:
# 1) NodePort (recomendado)
# 2) Port-forwarding 
# 3) Ambos
```

### ConfiguraciÃ³n de la AplicaciÃ³n RaÃ­z
```bash
# Aplicar la aplicaciÃ³n raÃ­z (App of Apps)
sudo microk8s kubectl apply -f argo-cd/app.yaml

# Verificar aplicaciones
sudo microk8s kubectl get applications -n argocd
```

### Despliegue del Stack de MonitorizaciÃ³n
```bash
# Desplegar Prometheus y Grafana
chmod +x deploy-monitoring.sh
sudo ./deploy-monitoring.sh

# Verificar despliegue
sudo microk8s kubectl get pods -n mlops-project | grep -E 'prometheus|grafana'
```

### VerificaciÃ³n del Despliegue Completo
```bash
# Estado de todas las aplicaciones Argo CD
sudo microk8s kubectl get applications -n argocd

# Estado de pods en mlops-project
sudo microk8s kubectl get pods -n mlops-project

# Estado de servicios
sudo microk8s kubectl get services -n mlops-project

# Logs de Argo CD
sudo microk8s kubectl logs -f deployment/argocd-server -n argocd
```

## GestiÃ³n y Mantenimiento

### Scripts de Utilidad

**Stop Port Forwards:**
```bash
# Detener todos los port-forwards activos
./stop_port_forwards.sh
```

**Cleanup Completo:**
```bash
# Eliminar Prometheus y Grafana
sudo ./cleanall.sh
```

### Comandos de Troubleshooting

**Verificar sincronizaciÃ³n de Argo CD:**
```bash
# Estado de sync de todas las apps
sudo microk8s kubectl get applications -n argocd -o wide

# Detalles de una aplicaciÃ³n especÃ­fica
sudo microk8s kubectl describe application fastapi-app -n argocd
```

**Forzar sincronizaciÃ³n:**
```bash
# Sincronizar manualmente una aplicaciÃ³n
argocd app sync fastapi-app
```

**Verificar mÃ©tricas:**
```bash
# Test de endpoints de mÃ©tricas
curl http://<NODE_IP>:30601/metrics  # FastAPI
curl http://<NODE_IP>:31090/targets  # Prometheus targets
```

## Estructura de Archivos

```
03_Tercera_maquina/
â”œâ”€â”€ argo-cd/
â”‚   â”œâ”€â”€ app.yaml                   # AplicaciÃ³n raÃ­z (App of Apps)
â”‚   â”œâ”€â”€ install.yaml               # Imagen oficial de Argo CD    
â”‚   â””â”€â”€ apps/
â”‚       â”œâ”€â”€ fastapi.yaml           # App Argo CD para FastAPI
â”‚       â”œâ”€â”€ gradio.yaml            # App Argo CD para Gradio
â”‚       â”œâ”€â”€ mlflow.yaml            # App Argo CD para MLflow
â”‚       â””â”€â”€ monitoring.yaml        # App Argo CD para monitorizaciÃ³n
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yaml            # Deployment + ConfigMap Prometheus
â”‚   â””â”€â”€ grafana.yaml              # Deployment + Service Grafana
â”œâ”€â”€ deploy-argo.sh                # Script instalaciÃ³n Argo CD
â”œâ”€â”€ deploy-monitoring.sh          # Script instalaciÃ³n monitoring
â”œâ”€â”€ cleanall.sh                   # Script limpieza
â””â”€â”€ stop_port_forwards.sh         # Script stop port-forwards
```

## Flujo GitOps

![alt text](./Imagenes/image-1.png)

## CaracterÃ­sticas Avanzadas

### ConfiguraciÃ³n de Repository
- **Repository URL**: `https://github.com/miguelhzuniga/mlops-repo.git`
- **Target Revision**: `master`
- **Auto-sync**: Habilitado con `prune` y `selfHeal`
- **Directory Recursion**: Habilitado para apps anidadas

### PolÃ­ticas de SincronizaciÃ³n
- **Prune**: Elimina recursos no declarados en Git
- **Self Heal**: Revierte cambios manuales no autorizados
- **Automated Sync**: SincronizaciÃ³n automÃ¡tica al detectar cambios

### Observabilidad Integrada
- **Target Discovery**: Prometheus configura targets automÃ¡ticamente
- **Service Monitoring**: Health checks de todos los componentes
- **Resource Monitoring**: CPU, memoria y network de pods
- **Application Metrics**: MÃ©tricas custom de FastAPI y Gradio



## ğŸ”„ CI/CD Pipeline Completo

### GitHub Actions - Continuous Integration

El sistema implementa un pipeline de CI/CD completamente automatizado que gestiona desde la construcciÃ³n de imÃ¡genes hasta el despliegue en producciÃ³n.

---
![alt text](./Imagenes/git_action.png)
---
![alt text](./Imagenes/git_action2.png)
---
![alt text](./Imagenes/git_action3.png)
---
#### Workflow Principal: `mlops-images.yml`

**Triggers Configurados:**
```yaml
on:
  push:
    branches: [ master ]
    paths:
      - 'proyecto-04/01_Primera_maquina/**'
      - 'proyecto-04/02_Segunda_maquina/**'
  pull_request:
    branches: [ master ]
```

#### Jobs del Pipeline

**1. Build Jobs (Paralelos):**
- `build-airflow`: Construye imagen personalizada de Airflow
- `build-mlflow`: Construye imagen personalizada de MLflow
- `build-fastapi`: Construye imagen del API de inferencia
- `build-Gradio`: Construye imagen de la interfaz de usuario

**2. Deploy Jobs (Secuenciales):**
- `actualizar-manifiestos`: Actualiza automÃ¡ticamente manifiestos K8s
- `resumen-despliegue`: Genera reporte completo del pipeline

#### CaracterÃ­sticas TÃ©cnicas del Pipeline

**Multi-arquitectura:** Soporte nativo para `linux/amd64` y `linux/arm64`
```dockerfile
platforms: linux/amd64,linux/arm64
```

**CachÃ© Inteligente:** OptimizaciÃ³n de builds con GitHub Actions cache
```yaml
cache-from: type=gha,scope=fastapi
cache-to: type=gha,mode=max,scope=fastapi
```

**Versionado AutomÃ¡tico:** Tags Ãºnicos basados en timestamp + commit SHA
```bash
# Formato: YYYYMMDD-{short-sha}
# Ejemplo: 20250603-a1b2c3d
```

#### ActualizaciÃ³n AutomÃ¡tica de Manifiestos

El pipeline incluye un job especializado que:

1. **Detecta manifiestos**: Identifica archivos YAML de Kubernetes a actualizar
2. **Actualiza imÃ¡genes**: Modifica solo las lÃ­neas de imagen con nuevos tags
3. **Valida cambios**: Verifica que las actualizaciones fueron correctas
4. **Commit automÃ¡tico**: Sube cambios con mensaje descriptivo
5. **Trigger Argo CD**: Los cambios disparan sincronizaciÃ³n automÃ¡tica

```bash
# Ejemplo de actualizaciÃ³n automÃ¡tica
sed -i -E "s|(image:.*fastapi-houses):.*|\1:20250603-a1b2c3d|g" \
  proyecto-04/02_Segunda_maquina/api/fastapi/fastapi-deployment.yaml
```

### IntegraciÃ³n GitHub Actions + Argo CD

#### Flujo End-to-End

![alt text](./Imagenes/image-4.png)

#### ConfiguraciÃ³n de Secrets

**GitHub Repository Secrets:**
```bash
DOCKER_USERNAME= Usuario_dockerhub
DOCKER_PASSWORD= Personal_access_token
GITHUB_TOKEN=ghp_xxx  # Auto-generado por GitHub
```

#### Ejemplo de EjecuciÃ³n Exitosa

```bash
ğŸš€ Resumen de EjecuciÃ³n del Pipeline MLOps
==================================================

ğŸ“Š Resultados de Build:
â€¢ Build Airflow:          âœ… success
â€¢ Build MLflow:           âœ… success
â€¢ Build FastAPI:          âœ… success
â€¢ Build Gradio:        âœ… success
â€¢ Actualizar Manifiestos: âœ… success

ğŸ“¦ ImÃ¡genes Docker Publicadas:
â€¢ miguelhzuniga/airflow-houses:20250603-a1b2c3d
â€¢ miguelhzuniga/mlflow-houses:20250603-a1b2c3d
â€¢ miguelhzuniga/fastapi-houses:20250603-a1b2c3d
â€¢ miguelhzuniga/Gradio-houses:20250603-a1b2c3d

ğŸ¯ Despliegue AutomÃ¡tico:
âœ… Manifiestos K8s actualizados
ğŸ”„ Argo CD sincronizando en ~3 minutos
```
---
![alt text](./Imagenes/git_action4.png)
---

## ğŸŒ ConfiguraciÃ³n de Red y Acceso

### Mapa de Servicios y Puertos

| Servicio | Puerto | URL | Protocolo | Estado |
|----------|--------|-----|-----------|---------|
| **Airflow WebUI** | 8080 | `http://10.43.101.175:8080` | HTTP | Docker Compose |
| **MLflow Tracking** | 30500 | `http://10.43.101.175:30500` | HTTP | Kubernetes |
| **FastAPI Docs** | 30601 | `http://10.43.101.202:30601/docs` | HTTP | Kubernetes |
| **Gradio UI** | 30602 | `http://10.43.101.202:30602` | HTTP | Kubernetes |
| **Argo CD Dashboard** | 30080 | `http://10.43.101.206:30080` | HTTP | Kubernetes |
| **Prometheus UI** | 31090 | `http://10.43.101.206:31090` | HTTP | Kubernetes |
| **Grafana Dashboards** | 31300 | `http://10.43.101.206:31300` | HTTP | Kubernetes |
| **MinIO Console** | 30901 | `http://10.43.101.175:30901` | HTTP | Kubernetes |
| **PgAdmin** | 5050 | `http://10.43.101.175:5050` | HTTP | Docker Compose |

### Credenciales de Acceso

| Servicio | Usuario | ContraseÃ±a | Notas |
|----------|---------|------------|-------|
| Airflow | `admin` | `admin` | Configurado en docker-compose |
| Argo CD | `admin` | `Generada por el despliegue` | ConfiguraciÃ³n inicial |
| Grafana | `admin` | `admin` | Dashboards preconfigurados |
| MinIO | `adminuser` | `securepassword123` | S3-compatible storage |
| PgAdmin | `admin@admin.com` | `admin` | PostgreSQL management |

### ComunicaciÃ³n Entre Servicios
![alt text](./Imagenes/image-3.png)



## ğŸ“‹ GuÃ­a de Despliegue Completo

### Pre-requisitos del Sistema

```bash
# Verificar versiones mÃ­nimas
kubectl version --client    # v1.20+
docker version             # v20.10+
microk8s version           # v1.20+

# Recursos mÃ­nimos recomendados
# CPU: 8 cores
# RAM: 16GB
# Storage: 100GB
# Network: 1Gbps
```

### InstalaciÃ³n Paso a Paso

#### 1. PreparaciÃ³n del Entorno

```bash
# Clonar repositorio
git clone https://github.com/miguelhzuniga/mlops-repo.git
cd mlops-repo/proyecto-04

# Configurar MicroK8s
sudo snap install microk8s --classic
sudo usermod -a -G microk8s $USER
newgrp microk8s

# Habilitar addons necesarios
microk8s enable dns dashboard storage ingress
```

#### 2. Despliegue MÃ¡quina 1 (Data Pipeline)

```bash
cd 01_Primera_maquina

# Desplegar Airflow
cd airflow
chmod +x deploy.sh
./deploy.sh

# Verificar Airflow
docker-compose ps
echo "Airflow UI: http://localhost:8080"

# Desplegar MLflow
cd ../mlflow
chmod +x deploy.sh
./deploy.sh

# Verificar MLflow
kubectl get pods -n mlops-project
echo "MLflow UI: http://localhost:30500"
```
#### 3. Despliegue MÃ¡quina 2 (Data Pipeline)

```bash
cd 02_Primera_maquina

# Desplegar FastApi & Gradio
cd api
chmod +x deploy.sh
./deploy.sh
```
#### 4. Configurar GitHub Actions

```bash
# En GitHub Repository Settings > Secrets:
# 1. Ir a Settings > Secrets and variables > Actions
# 2. Crear secrets:
DOCKER_USERNAME=Usuario_dockerhub
DOCKER_PASSWORD=Token_dockerhub

# 3. Verificar workflow
git push origin master (Tambien se puede hacer push a una rama y merge a master/main)
# Monitorear en: GitHub > Actions tab
```

#### 5. Despliegue MÃ¡quina 3 (GitOps)

```bash
cd 03_Tercera_maquina

# Instalar Argo CD
chmod +x deploy-argo.sh
./deploy-argo.sh

# Configurar aplicaciÃ³n raÃ­z
kubectl apply -f argo-cd/app.yaml

# Desplegar monitoring
chmod +x deploy-monitoring.sh
./deploy-monitoring.sh

# Verificar instalaciÃ³n
kubectl get applications -n argocd
echo "Argo CD UI: http://localhost:30080"
```

#### 5. VerificaciÃ³n Final

```bash
# Estado general del sistema
kubectl get pods --all-namespaces
kubectl get services --all-namespaces

# Verificar aplicaciones en Argo CD
kubectl get applications -n argocd

# Test de conectividad
curl http://localhost:30601/health
curl http://localhost:30500/health
```

### VerificaciÃ³n de Servicios

#### Health Check Script

```bash
#!/bin/bash
echo "ğŸ” Verificando estado de servicios MLOps..."

services=(
    "http://localhost:8080|Airflow"
    "http://localhost:30500|MLflow"
    "http://localhost:30601/health|FastAPI"
    "http://localhost:30602|Gradio"
    "http://localhost:30080|Argo CD"
    "http://localhost:31090|Prometheus"
    "http://localhost:31300|Grafana"
)

for service in "${services[@]}"; do
    url=$(echo $service | cut -d'|' -f1)
    name=$(echo $service | cut -d'|' -f2)
    
    if curl -s --max-time 5 $url > /dev/null; then
        echo "âœ… $name: OK"
    else
        echo "âŒ $name: FAIL"
    fi
done
```

---

## ğŸ› ï¸ Troubleshooting y Mantenimiento

### Problemas Comunes y Soluciones

#### GitHub Actions

**âŒ Error: "Failed to push to DockerHub"**
```bash
# Verificar configuraciÃ³n de secrets
# GitHub > Settings > Secrets and variables > Actions
DOCKER_USERNAME=tu_usuario  # Sin errores tipogrÃ¡ficos
DOCKER_PASSWORD=tu_token    # Usar PAT, no password regular

# Verificar permisos del token en DockerHub
# DockerHub > Account Settings > Security > New Access Token
# Scope: Read, Write, Delete
```

**âŒ Error: "Manifest update failed"**
```bash
# Verificar que manifiestos existen
find . -name "*-deployment.yaml" -type f

# Verificar formato de imagen en YAML
grep -n "image:" proyecto-04/02_Segunda_maquina/api/fastapi/fastapi-deployment.yaml
# Formato correcto: image: usuario/imagen:tag
```

#### Argo CD

**âŒ Application "OutOfSync"**
```bash
# SincronizaciÃ³n manual forzada
argocd app sync mlops-root-app --force

# Ver diferencias especÃ­ficas
argocd app diff fastapi-app

# Refresh y re-sync
argocd app get fastapi-app --refresh
argocd app sync fastapi-app
```

**âŒ Application "Degraded"**
```bash
# Verificar pods
kubectl get pods -n mlops-project
kubectl describe pod <pod-name> -n mlops-project

# Ver logs de aplicaciÃ³n
kubectl logs deployment/fastapi-housing -n mlops-project --tail=50

# Verificar recursos
kubectl top pods -n mlops-project
```

#### MLflow

**âŒ Error: "Connection to MinIO failed"**
```bash
# Verificar pods MinIO
kubectl get pods -n mlops-project | grep minio

# Test de conectividad
kubectl exec -it deployment/mlflow -n mlops-project -- \
  python -c "
import boto3
s3 = boto3.client('s3', endpoint_url='http://minio:9000')
print(s3.list_buckets())
"
```

#### FastAPI

**âŒ Error: "Model loading failed"**
```bash
# Verificar conexiÃ³n a MLflow
kubectl logs deployment/fastapi-housing -n mlops-project | grep -i mlflow

# Test manual de carga de modelo
kubectl exec -it deployment/fastapi-housing -n mlops-project -- \
  python -c "
import mlflow
mlflow.set_tracking_uri('http://mlflow:5000')
client = mlflow.tracking.MlflowClient()
print(client.search_registered_models())
"
```

### Comandos de DiagnÃ³stico

#### Sistema General
```bash
# Estado del cluster
kubectl cluster-info
kubectl get nodes -o wide

# Uso de recursos
kubectl top nodes
kubectl top pods --all-namespaces

# Events del sistema
kubectl get events --sort-by=.metadata.creationTimestamp

# Logs de sistema
journalctl -u snap.microk8s.daemon -f
```

#### Aplicaciones EspecÃ­ficas
```bash
# Airflow
docker-compose logs airflow-webserver --tail=50
docker-compose logs airflow-scheduler --tail=50

# MLflow
kubectl logs deployment/mlflow -n mlops-project --tail=50

# FastAPI
kubectl logs deployment/fastapi-housing -n mlops-project --tail=50

# Argo CD
kubectl logs deployment/argocd-application-controller -n argocd --tail=50
```

### Mejores PrÃ¡cticas Operacionales

#### Desarrollo
- âœ… Usar feature branches para nuevas funcionalidades
- âœ… Implementar tests unitarios antes de push
- âœ… Revisar logs de GitHub Actions antes de merge
- âœ… Mantener Dockerfiles optimizados (multi-stage, .dockerignore)

#### ProducciÃ³n
- âœ… Monitorear dashboards de Grafana diariamente
- âœ… Configurar alertas para mÃ©tricas crÃ­ticas
- âœ… Realizar backups regulares de PostgreSQL y MinIO
- âœ… Rotar secrets y tokens mensualmente
- âœ… Mantener documentaciÃ³n de runbooks actualizada

#### Seguridad
- âœ… Usar Kubernetes secrets para datos sensibles
- âœ… Implementar Network Policies para aislar servicios
- âœ… Configurar RBAC granular en Argo CD
- âœ… Auditar accesos a sistemas regularmente
- âœ… Mantener imÃ¡genes actualizadas con patches de seguridad

---

## ğŸ“ˆ MÃ©tricas y Observabilidad

### KPIs del Sistema

#### MÃ©tricas de Rendimiento
| MÃ©trica | Target | Alerta | DescripciÃ³n |
|---------|--------|--------|-------------|
| API Latency P95 | < 1s | > 2s | Tiempo de respuesta predicciones |
| API Throughput | 50-100 RPS | < 10 RPS | Solicitudes por segundo |
| Error Rate | < 0.1% | > 1% | Porcentaje de errores HTTP |
| Model Accuracy | RÂ² > 0.85 | RÂ² < 0.8 | PrecisiÃ³n del modelo activo |
| Uptime | > 99.9% | < 99% | Disponibilidad del servicio |

#### MÃ©tricas de Negocio
- **Predicciones diarias**: Volumen de inferencias
- **Accuracy trend**: EvoluciÃ³n de precisiÃ³n del modelo
- **Feature importance**: Importancia de variables en predicciones
- **Data drift**: Cambios en distribuciÃ³n de datos de entrada
- **Model retraining frequency**: Frecuencia de nuevos entrenamientos

### Dashboards de Grafana

#### 1. MLOps Operations Dashboard
- MÃ©tricas de pipeline de datos (Airflow)
- Estado de experimentos MLflow
- Performance de modelos en producciÃ³n
- Alertas activas del sistema

#### 2. API Performance Dashboard
- Latencia y throughput de FastAPI
- DistribuciÃ³n de cÃ³digos de respuesta HTTP
- Uso de recursos (CPU, memoria)
- Geographic distribution de requests

#### 3. Business Intelligence Dashboard
- Trends de precios predichos vs reales
- AnÃ¡lisis de caracterÃ­sticas mÃ¡s influyentes
- ROI de predicciones automatizadas
- User engagement con Gradio

---

## ğŸ“ GuÃ­a de Uso para Usuarios Finales

### Para Data Scientists

#### ExperimentaciÃ³n con MLflow
```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor

# Configurar tracking
mlflow.set_tracking_uri("http://10.43.101.206:30500")
mlflow.set_experiment("realtor_price_prediction")

with mlflow.start_run():
    # Entrenar modelo
    model = RandomForestRegressor(n_estimators=100)
    model.fit(X_train, y_train)
    
    # Log parÃ¡metros y mÃ©tricas
    mlflow.log_param("n_estimators", 100)
    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("r2", r2)
    
    # Registrar modelo
    mlflow.sklearn.log_model(model, "model")
```

#### PromociÃ³n de Modelos
```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Encontrar mejor modelo
best_run = client.search_runs(
    experiment_ids=["1"],
    order_by=["metrics.rmse ASC"],
    max_results=1
)[0]

# Registrar como nuevo modelo
mlflow.register_model(
    f"runs:/{best_run.info.run_id}/model",
    "realtor_price_model"
)

# Promover a producciÃ³n
client.transition_model_version_stage(
    name="realtor_price_model",
    version=1,
    stage="Production"
)
```

### Para DevOps Engineers

#### Despliegue de Nueva VersiÃ³n
```bash
# 1. Actualizar cÃ³digo y push
git add .
git commit -m "feat: improved model accuracy"
git push origin master

# 2. Monitorear GitHub Actions
gh run watch

# 3. Verificar sincronizaciÃ³n Argo CD
argocd app get fastapi-app
argocd app sync fastapi-app

# 4. Validar deployment
kubectl rollout status deployment/fastapi-housing -n mlops-project
curl http://localhost:30601/health
```

#### Rollback de Emergencia
```bash
# Rollback vÃ­a Argo CD
argocd app rollback fastapi-app --revision 2

# O rollback directo en Kubernetes
kubectl rollout undo deployment/fastapi-housing -n mlops-project

# Verificar estado
kubectl get pods -n mlops-project
```

### Para Business Users

#### Uso de Gradio Interface

1. **Acceder a la aplicaciÃ³n**: `http://10.43.101.202:30602`

2. **Ingresar datos de propiedad**:
   - InformaciÃ³n bÃ¡sica: habitaciones, baÃ±os, tamaÃ±o
   - UbicaciÃ³n: ciudad, estado, cÃ³digo postal
   - CaracterÃ­sticas: tamaÃ±o del lote, fecha venta anterior

3. **Interpretar resultados**:
   - Precio estimado con intervalo de confianza
   - SHAP explanation de factores influyentes
   - ComparaciÃ³n con propiedades similares

4. **Monitorear rendimiento**:
   - Dashboard de mÃ©tricas de modelo
   - HistÃ³rico de predicciones
   - Trends de mercado inmobiliario

---


### TecnologÃ­as Utilizadas

- **Apache Airflow**: [https://airflow.apache.org/docs/](https://airflow.apache.org/docs/)
- **MLflow**: [https://mlflow.org/docs/latest/](https://mlflow.org/docs/latest/)
- **FastAPI**: [https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/)
- **Gradio**: [https://docs.Gradio.io/](https://docs.Gradio.io/)
- **Argo CD**: [https://argo-cd.readthedocs.io/](https://argo-cd.readthedocs.io/)
- **Prometheus**: [https://prometheus.io/docs/](https://prometheus.io/docs/)
- **Grafana**: [https://grafana.com/docs/](https://grafana.com/docs/)
- **Kubernetes**: [https://kubernetes.io/docs/](https://kubernetes.io/docs/)


## ğŸ‘¥ Equipo de Desarrollo

- **Luis Frontuso** 
- **Miguel ZuÃ±iga** 
- **Camilo Serrano** 

